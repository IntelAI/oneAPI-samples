{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481b71e-1f69-4574-b483-cb8a62930f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Copyright © 2020 Intel Corporation\n",
    "# \n",
    "# SPDX-License-Identifier: MIT\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d6b97-e1f5-4dc8-926d-b3ccb7a141e6",
   "metadata": {},
   "source": [
    "# `Intel® Extension for PyTorch* Getting Started and AutoMixedPrecision Feature` Sample\n",
    "\n",
    "\n",
    "\n",
    "## About Intel Extension for PyTorch\n",
    "\n",
    "PyTorch* is a very popular framework for deep learning, while also compute-heavy package demanding perfromance optimizations. Intel and Facebook* having been collaborating to boost PyTorch* CPU Performance. The official PyTorch has been optimized using oneAPI Deep Neural Network Library (oneDNN) primitives by default.\n",
    "\n",
    "To deliver the latest and greatest  optimizations, Intel Optimizations for Pytorch offers accelerations beyond the stock Pytorch via **Intel® Extension for PyTorch***(IPEX). IPEX is a Python package to extend the official PyTorch with optimizations for extra performance boost on Intel hardware. Most of the optimizations will be included in stock PyTorch releases eventually, and the intention of the extension is to deliver up-to-date features and optimizations for PyTorch on Intel hardware, examples include AVX-512 Vector Neural Network Instructions (AVX512 VNNI) and Intel® Advanced Matrix Extensions (Intel® AMX).\n",
    "\n",
    "More detailed tutorials are available at [Intel® Extension for PyTorch* online document website](https://intel.github.io/intel-extension-for-pytorch/).\n",
    "\n",
    "## Purpose\n",
    "This sample code shows how to get started with Intel Extension for PyTorch as well as how to use AutoMixedPrecision with Intel Extension for PyTorch.\n",
    "\n",
    "## Sample Table of Contents\n",
    "1. [Intel® Extension for PyTorch* Getting Started Sample](#sec-gs)\n",
    "2. [Intel® Extension for PyTorch* Auto Mixed Precision Sample](#sec-amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945c4e9-eb4e-4ded-8776-11b43df63971",
   "metadata": {},
   "source": [
    "<a id=\"sec-gs\"></a>\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d948ec-4a90-4b1a-9f9c-93fc430eafff",
   "metadata": {},
   "source": [
    "If you want to explore Intel Extension for PyTorch, you just need to convert the model and input tensors to the extension device, then the extension will be enabled automatically. Take an example, the code as follows is a model without the extension.\n",
    "\n",
    "\n",
    "**Please run this sample in the Intel PyTorch & Quantization Jupyter Kernel environment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee02134-6598-4170-89d2-6a1211caf5b9",
   "metadata": {},
   "source": [
    "### PyTorch Model without Intel Extension for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a350e-47ab-4189-a016-df003e1df756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear(input)\n",
    "\n",
    "input = torch.randn(2, 4)\n",
    "model = Model()\n",
    "res = model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e39ecb-2a24-4923-84e1-8e913cd6078f",
   "metadata": {},
   "source": [
    "### PyTorch Model with Intel Extension for PyTorch\n",
    "\n",
    "You just need to transform the above python script with **a couple lines of code** as follows and then the extension will be enabled and accelerate the computation automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a42a04-d86a-4082-854a-0cc16de6284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import Extension\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear(input)\n",
    "\n",
    "####################IPEX code changes#############################\n",
    "# Convert the input tensor to the Extension device\n",
    "input = torch.randn(2, 4).to()\n",
    "# Convert the model to the Extension device\n",
    "model = Model().to()\n",
    "##################################################################\n",
    "\n",
    "res = model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f348d-61a1-479f-b397-60f6997ce535",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"sec-amp\"></a>\n",
    "## Automatically Mix Precision\n",
    "\n",
    "In addition, Intel Extension for PyTorch supports the mixed precision. It means that some operators of a model may run with Float32 and some other operators may run with BFloat16 or INT8 to accelerate inference workload.\n",
    "\n",
    "Traditionally if you want to run a model with a low precision type, you need to convert the parameters and the input tensors to the low precision type manually. And if the model contains some operators that do not support the low precision type, then you have to convert back to Float32. Round after round until the model can run normally.\n",
    "\n",
    "**IPEX can simplify this procedure. You just need to enable the auto-mix-precision as follows, then you can benefit from the low precision. Currently, the extension  supports BFloat16.**\n",
    "\n",
    "### BFloat16 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec4142d-94ed-4b24-bfbb-ff6b68cd85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import intel_extension_for_pytorch as ipex\n",
    "# Automatically mix precision\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear(input)\n",
    "with torch.cpu.amp.autocast(), torch.no_grad():\n",
    "    input = torch.randn(2, 4).to()\n",
    "    model = Model().to()\n",
    "\n",
    "    res = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dbfb50-cad2-43aa-8e7a-ade307cc7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[CODE_SAMPLE_COMPLETED_SUCCESFULLY]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436f8ae-494a-4ee2-8b88-e1b34e11cbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel PyTorch & Quantization",
   "language": "python",
   "name": "oneapi-aikit-dlpackage-with-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
